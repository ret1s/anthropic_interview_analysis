{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anthropic Interviewer dataset\n",
        "\n",
        "Quickstart notebook to pull the transcripts from the Hugging Face dataset and do light inspection.\n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataset:** `Anthropic/AnthropicInterviewer` on Hugging Face.\n",
        "- Interview transcripts from 1,250 professionals (workforce=1,000, creatives=125, scientists=125).\n",
        "- Data is CC-BY; code MIT. Public dataset, so no auth token needed for reading.\n",
        "\n",
        "Run the install cell once per environment, then execute the rest.\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d962102e",
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "%pip install -q pandas huggingface_hub openai scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b09d095",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SPLITS = {\n",
        "    \"workforce\": \"interview_transcripts/workforce_transcripts.csv\",\n",
        "    \"creatives\": \"interview_transcripts/creatives_transcripts.csv\",\n",
        "    \"scientists\": \"interview_transcripts/scientists_transcripts.csv\",\n",
        "}\n",
        "BASE_PATH = \"hf://datasets/Anthropic/AnthropicInterviewer/\"\n",
        "\n",
        "def load_split(name: str) -> pd.DataFrame:\n",
        "    path = BASE_PATH + SPLITS[name]\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"split\"] = name\n",
        "    return df\n",
        "\n",
        "dfs = {name: load_split(name) for name in SPLITS}\n",
        "for name, df in dfs.items():\n",
        "    cols = \", \".join(df.columns)\n",
        "    print(f\"{name:10} {df.shape[0]:4} rows | columns: {cols}\")\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1493574c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick look at the workforce split\n",
        "dfs[\"workforce\"].head()\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cda3f76",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample rows across all splits\n",
        "all_df = pd.concat(dfs.values(), ignore_index=True)\n",
        "all_df.sample(5, random_state=42)[[\"transcript_id\", \"split\", \"text\"]]\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ec7fc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rough length stats by split (character count of transcript text)\n",
        "all_df = all_df.copy()\n",
        "all_df[\"text_length\"] = all_df[\"text\"].str.len()\n",
        "all_df.groupby(\"split\")[\"text_length\"].describe()[[\"count\", \"mean\", \"min\", \"max\"]]\n",
        "            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ad7624",
      "metadata": {},
      "source": [
        "### Per-split descriptive stats\n",
        "Add word-level and character-level summaries to see distribution differences per group.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f441605",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word-level descriptive stats by split\n",
        "all_df = pd.concat(dfs.values(), ignore_index=True)\n",
        "all_df = all_df.assign(\n",
        "    word_count=all_df[\"text\"].str.split().str.len(),\n",
        "    char_count=all_df[\"text\"].str.len(),\n",
        ")\n",
        "summary = (\n",
        "    all_df.groupby(\"split\")[[\"word_count\", \"char_count\"]]\n",
        "    .agg([\"count\", \"mean\", \"median\", \"min\", \"max\"])\n",
        "    .round(2)\n",
        ")\n",
        "summary\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91d5784",
      "metadata": {},
      "source": [
        "### Top keywords per group (TF-IDF)\n",
        "Rough sense of distinctive vocabulary by group. Adjust `top_n` or stop words as needed.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75414c5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "top_n = 15\n",
        "results = {}\n",
        "for split, df in dfs.items():\n",
        "    vec = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "    matrix = vec.fit_transform(df[\"text\"])\n",
        "    scores = matrix.sum(axis=0).A1\n",
        "    terms = vec.get_feature_names_out()\n",
        "    order = scores.argsort()[::-1][:top_n]\n",
        "    results[split] = [(terms[i], float(scores[i])) for i in order]\n",
        "\n",
        "for split, items in results.items():\n",
        "    print(f\"\\n{split.title()} top {top_n} tf-idf terms:\")\n",
        "    for term, score in items:\n",
        "        print(f\"  {term:20s} {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df9c0c3",
      "metadata": {},
      "source": [
        "### LLM themes per group\n",
        "Set `OPENAI_API_KEY` in your environment. The cell below samples transcripts per split and asks a stronger model (default `gpt-4o`) for 5 themes with supporting evidence.\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ac9d06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"Set OPENAI_API_KEY in your environment before running this cell.\")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "separator = \"\\n\\n---\\n\\n\"\n",
        "\n",
        "def summarize_split(split: str, sample_size: int = 8, model: str = \"gpt-4o\") -> str:\n",
        "    subset = dfs[split].sample(sample_size, random_state=42)[\"text\"].tolist()\n",
        "    prompt = f\"\"\"\n",
        "You are analyzing qualitative interview transcripts from the {split} group.\n",
        "Extract 5 themes. For each theme, provide a short label and 1-2 bullet examples grounded in the text.\n",
        "Return concise markdown.\n",
        "\n",
        "Transcripts (each separated by ---):\n",
        "{separator.join(subset)}\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "for split in [\"workforce\", \"creatives\", \"scientists\"]:\n",
        "    print(f\"\\n### {split.title()} themes\\n\")\n",
        "    print(summarize_split(split))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66c7750",
      "metadata": {},
      "source": [
        "### Save per-group LLM themes to markdown\n",
        "Writes the generated themes to `analysis/llm_group_analysis.md` for easy reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9523e174",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"analysis\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "out_path = output_dir / \"llm_group_analysis.md\"\n",
        "\n",
        "sections = []\n",
        "for split in [\"workforce\", \"creatives\", \"scientists\"]:\n",
        "    sections.append(f\"## {split.title()} themes\\n\")\n",
        "    sections.append(summarize_split(split))\n",
        "\n",
        "content = \"\\n\\n\".join(sections)\n",
        "out_path.write_text(content)\n",
        "print(f\"Wrote {out_path} ({len(content)} chars)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da76b265",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: persist the three splits locally in data/\n",
        "output_dir = Path(\"data\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "for name, df in dfs.items():\n",
        "    dest = output_dir / f\"{name}_transcripts.csv\"\n",
        "    df.to_csv(dest, index=False)\n",
        "    print(f\"Wrote {dest}\")\n",
        "            \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
